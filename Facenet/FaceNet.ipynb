{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from Generator import DataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_resnet_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_1a_3x3 (Conv2D)          (None, 59, 59, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_1a_3x3_BatchNorm (BatchN (None, 59, 59, 32)   96          Conv2d_1a_3x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_1a_3x3_Activation (Activ (None, 59, 59, 32)   0           Conv2d_1a_3x3_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2a_3x3 (Conv2D)          (None, 57, 57, 32)   9216        Conv2d_1a_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2a_3x3_BatchNorm (BatchN (None, 57, 57, 32)   96          Conv2d_2a_3x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2a_3x3_Activation (Activ (None, 57, 57, 32)   0           Conv2d_2a_3x3_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2b_3x3 (Conv2D)          (None, 57, 57, 64)   18432       Conv2d_2a_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2b_3x3_BatchNorm (BatchN (None, 57, 57, 64)   192         Conv2d_2b_3x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_2b_3x3_Activation (Activ (None, 57, 57, 64)   0           Conv2d_2b_3x3_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_3a_3x3 (MaxPooling2D)   (None, 28, 28, 64)   0           Conv2d_2b_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_3b_1x1 (Conv2D)          (None, 28, 28, 80)   5120        MaxPool_3a_3x3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_3b_1x1_BatchNorm (BatchN (None, 28, 28, 80)   240         Conv2d_3b_1x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_3b_1x1_Activation (Activ (None, 28, 28, 80)   0           Conv2d_3b_1x1_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4a_3x3 (Conv2D)          (None, 26, 26, 192)  138240      Conv2d_3b_1x1_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4a_3x3_BatchNorm (BatchN (None, 26, 26, 192)  576         Conv2d_4a_3x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4a_3x3_Activation (Activ (None, 26, 26, 192)  0           Conv2d_4a_3x3_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4b_3x3 (Conv2D)          (None, 12, 12, 256)  442368      Conv2d_4a_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4b_3x3_BatchNorm (BatchN (None, 12, 12, 256)  768         Conv2d_4b_3x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Conv2d_4b_3x3_Activation (Activ (None, 12, 12, 256)  0           Conv2d_4b_3x3_BatchNorm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   8192        Conv2d_4b_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_1_Branch_2_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_1_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   8192        Conv2d_4b_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_1_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_1_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_1_Branch_2_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_1_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_1_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_0_Conv2d_1x1 ( (None, 12, 12, 32)   8192        Conv2d_4b_3x3_Activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_1_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   9216        Block35_1_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_0_Conv2d_1x1_B (None, 12, 12, 32)   96          Block35_1_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_1_Branch_1_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   96          Block35_1_Branch_2_Conv2d_0c_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_0_Conv2d_1x1_A (None, 12, 12, 32)   0           Block35_1_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_1_Branch_1_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   0           Block35_1_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Concatenate (Concaten (None, 12, 12, 96)   0           Block35_1_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block35_1_Branch_1_Conv2d_0b_3x3_\n",
      "                                                                 Block35_1_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Conv2d_1x1 (Conv2D)   (None, 12, 12, 256)  24832       Block35_1_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 12, 12, 256)  0           Block35_1_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 12, 12, 256)  0           Conv2d_4b_3x3_Activation[0][0]   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_1_Activation (Activatio (None, 12, 12, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_1_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_2_Branch_2_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_2_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_1_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_2_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_2_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_2_Branch_2_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_2_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_2_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_0_Conv2d_1x1 ( (None, 12, 12, 32)   8192        Block35_1_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_2_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   9216        Block35_2_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_0_Conv2d_1x1_B (None, 12, 12, 32)   96          Block35_2_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_2_Branch_1_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   96          Block35_2_Branch_2_Conv2d_0c_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_0_Conv2d_1x1_A (None, 12, 12, 32)   0           Block35_2_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_2_Branch_1_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   0           Block35_2_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Concatenate (Concaten (None, 12, 12, 96)   0           Block35_2_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block35_2_Branch_1_Conv2d_0b_3x3_\n",
      "                                                                 Block35_2_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Conv2d_1x1 (Conv2D)   (None, 12, 12, 256)  24832       Block35_2_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 12, 12, 256)  0           Block35_2_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 12, 12, 256)  0           Block35_1_Activation[0][0]       \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_2_Activation (Activatio (None, 12, 12, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_2_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_3_Branch_2_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_3_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_2_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_3_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_3_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_3_Branch_2_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_3_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_3_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_0_Conv2d_1x1 ( (None, 12, 12, 32)   8192        Block35_2_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_3_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   9216        Block35_3_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_0_Conv2d_1x1_B (None, 12, 12, 32)   96          Block35_3_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_3_Branch_1_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   96          Block35_3_Branch_2_Conv2d_0c_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_0_Conv2d_1x1_A (None, 12, 12, 32)   0           Block35_3_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_3_Branch_1_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   0           Block35_3_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Concatenate (Concaten (None, 12, 12, 96)   0           Block35_3_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block35_3_Branch_1_Conv2d_0b_3x3_\n",
      "                                                                 Block35_3_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Conv2d_1x1 (Conv2D)   (None, 12, 12, 256)  24832       Block35_3_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 12, 12, 256)  0           Block35_3_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 12, 256)  0           Block35_2_Activation[0][0]       \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_3_Activation (Activatio (None, 12, 12, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_3_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_4_Branch_2_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_4_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_3_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_4_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_4_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_4_Branch_2_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_4_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_4_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_0_Conv2d_1x1 ( (None, 12, 12, 32)   8192        Block35_3_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_4_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   9216        Block35_4_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_0_Conv2d_1x1_B (None, 12, 12, 32)   96          Block35_4_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_4_Branch_1_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   96          Block35_4_Branch_2_Conv2d_0c_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_0_Conv2d_1x1_A (None, 12, 12, 32)   0           Block35_4_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_4_Branch_1_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   0           Block35_4_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Concatenate (Concaten (None, 12, 12, 96)   0           Block35_4_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block35_4_Branch_1_Conv2d_0b_3x3_\n",
      "                                                                 Block35_4_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Conv2d_1x1 (Conv2D)   (None, 12, 12, 256)  24832       Block35_4_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 12, 12, 256)  0           Block35_4_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 12, 12, 256)  0           Block35_3_Activation[0][0]       \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_4_Activation (Activatio (None, 12, 12, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_4_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_5_Branch_2_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_5_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   8192        Block35_4_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_5_Branch_2_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   96          Block35_5_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_5_Branch_2_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0a_1x (None, 12, 12, 32)   0           Block35_5_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_5_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_0_Conv2d_1x1 ( (None, 12, 12, 32)   8192        Block35_4_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   9216        Block35_5_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   9216        Block35_5_Branch_2_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_0_Conv2d_1x1_B (None, 12, 12, 32)   96          Block35_5_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   96          Block35_5_Branch_1_Conv2d_0b_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   96          Block35_5_Branch_2_Conv2d_0c_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_0_Conv2d_1x1_A (None, 12, 12, 32)   0           Block35_5_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_1_Conv2d_0b_3x (None, 12, 12, 32)   0           Block35_5_Branch_1_Conv2d_0b_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Branch_2_Conv2d_0c_3x (None, 12, 12, 32)   0           Block35_5_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Concatenate (Concaten (None, 12, 12, 96)   0           Block35_5_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block35_5_Branch_1_Conv2d_0b_3x3_\n",
      "                                                                 Block35_5_Branch_2_Conv2d_0c_3x3_\n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Conv2d_1x1 (Conv2D)   (None, 12, 12, 256)  24832       Block35_5_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 12, 12, 256)  0           Block35_5_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 12, 12, 256)  0           Block35_4_Activation[0][0]       \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block35_5_Activation (Activatio (None, 12, 12, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1 (None, 12, 12, 192)  49152       Block35_5_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1 (None, 12, 12, 192)  576         Mixed_6a_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1 (None, 12, 12, 192)  0           Mixed_6a_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3 (None, 12, 12, 192)  331776      Mixed_6a_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3 (None, 12, 12, 192)  576         Mixed_6a_Branch_1_Conv2d_0b_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3 (None, 12, 12, 192)  0           Mixed_6a_Branch_1_Conv2d_0b_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3 (None, 5, 5, 384)    884736      Block35_5_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3 (None, 5, 5, 256)    442368      Mixed_6a_Branch_1_Conv2d_0b_3x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3 (None, 5, 5, 384)    1152        Mixed_6a_Branch_0_Conv2d_1a_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3 (None, 5, 5, 256)    768         Mixed_6a_Branch_1_Conv2d_1a_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3 (None, 5, 5, 384)    0           Mixed_6a_Branch_0_Conv2d_1a_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3 (None, 5, 5, 256)    0           Mixed_6a_Branch_1_Conv2d_1a_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a_Branch_2_MaxPool_1a_3x (None, 5, 5, 256)    0           Block35_5_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_6a (Concatenate)          (None, 5, 5, 896)    0           Mixed_6a_Branch_0_Conv2d_1a_3x3_A\n",
      "                                                                 Mixed_6a_Branch_1_Conv2d_1a_3x3_A\n",
      "                                                                 Mixed_6a_Branch_2_MaxPool_1a_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_1_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_1_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_1_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_1_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_1_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_1_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_1_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_1_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_1_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_1_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_1_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_1_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_1_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 5, 5, 896)    0           Block17_1_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 5, 5, 896)    0           Mixed_6a[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_1_Activation (Activatio (None, 5, 5, 896)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_1_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_2_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_2_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_2_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_2_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_2_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_1_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_2_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_2_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_2_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_2_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_2_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_2_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_2_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_2_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 5, 5, 896)    0           Block17_2_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 5, 5, 896)    0           Block17_1_Activation[0][0]       \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_2_Activation (Activatio (None, 5, 5, 896)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_2_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_3_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_3_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_3_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_3_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_3_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_2_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_3_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_3_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_3_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_3_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_3_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_3_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_3_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_3_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 5, 5, 896)    0           Block17_3_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 5, 5, 896)    0           Block17_2_Activation[0][0]       \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_3_Activation (Activatio (None, 5, 5, 896)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_3_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_4_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_4_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_4_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_4_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_4_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_3_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_4_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_4_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_4_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_4_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_4_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_4_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_4_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_4_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 5, 5, 896)    0           Block17_4_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 5, 5, 896)    0           Block17_3_Activation[0][0]       \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block17_4_Activation (Activatio (None, 5, 5, 896)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_4_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_5_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_5_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_5_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_5_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_5_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_4_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_5_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_5_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_5_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_5_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_5_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_5_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_5_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_5_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 5, 5, 896)    0           Block17_5_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5, 5, 896)    0           Block17_4_Activation[0][0]       \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_5_Activation (Activatio (None, 5, 5, 896)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_5_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_6_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_6_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_6_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_6_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_6_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_5_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_6_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_6_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_6_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_6_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_6_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_6_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_6_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_6_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 5, 5, 896)    0           Block17_6_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 5, 5, 896)    0           Block17_5_Activation[0][0]       \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_6_Activation (Activatio (None, 5, 5, 896)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_6_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_7_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_7_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_7_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_7_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_7_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_6_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_7_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_7_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_7_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_7_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_7_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_7_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_7_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_7_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 5, 5, 896)    0           Block17_7_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 5, 5, 896)    0           Block17_6_Activation[0][0]       \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_7_Activation (Activatio (None, 5, 5, 896)    0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_7_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_8_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_8_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_8_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_8_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_8_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_7_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_8_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_8_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_8_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_8_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_8_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_8_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_8_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_8_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 5, 5, 896)    0           Block17_8_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 5, 5, 896)    0           Block17_7_Activation[0][0]       \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_8_Activation (Activatio (None, 5, 5, 896)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    114688      Block17_8_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    384         Block17_9_Branch_1_Conv2d_0a_1x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0a_1x (None, 5, 5, 128)    0           Block17_9_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    114688      Block17_9_Branch_1_Conv2d_0a_1x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    384         Block17_9_Branch_1_Conv2d_0b_1x7[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0b_1x (None, 5, 5, 128)    0           Block17_9_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_0_Conv2d_1x1 ( (None, 5, 5, 128)    114688      Block17_8_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    114688      Block17_9_Branch_1_Conv2d_0b_1x7_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_0_Conv2d_1x1_B (None, 5, 5, 128)    384         Block17_9_Branch_0_Conv2d_1x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    384         Block17_9_Branch_1_Conv2d_0c_7x1[\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_0_Conv2d_1x1_A (None, 5, 5, 128)    0           Block17_9_Branch_0_Conv2d_1x1_Bat\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Branch_1_Conv2d_0c_7x (None, 5, 5, 128)    0           Block17_9_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Concatenate (Concaten (None, 5, 5, 256)    0           Block17_9_Branch_0_Conv2d_1x1_Act\n",
      "                                                                 Block17_9_Branch_1_Conv2d_0c_7x1_\n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Conv2d_1x1 (Conv2D)   (None, 5, 5, 896)    230272      Block17_9_Concatenate[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 5, 5, 896)    0           Block17_9_Conv2d_1x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 5, 5, 896)    0           Block17_8_Activation[0][0]       \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_9_Activation (Activatio (None, 5, 5, 896)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0a_1 (None, 5, 5, 128)    114688      Block17_9_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0a_1 (None, 5, 5, 128)    384         Block17_10_Branch_1_Conv2d_0a_1x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0a_1 (None, 5, 5, 128)    0           Block17_10_Branch_1_Conv2d_0a_1x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0b_1 (None, 5, 5, 128)    114688      Block17_10_Branch_1_Conv2d_0a_1x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0b_1 (None, 5, 5, 128)    384         Block17_10_Branch_1_Conv2d_0b_1x7\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0b_1 (None, 5, 5, 128)    0           Block17_10_Branch_1_Conv2d_0b_1x7\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_0_Conv2d_1x1  (None, 5, 5, 128)    114688      Block17_9_Activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0c_7 (None, 5, 5, 128)    114688      Block17_10_Branch_1_Conv2d_0b_1x7\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_0_Conv2d_1x1_ (None, 5, 5, 128)    384         Block17_10_Branch_0_Conv2d_1x1[0]\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0c_7 (None, 5, 5, 128)    384         Block17_10_Branch_1_Conv2d_0c_7x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_0_Conv2d_1x1_ (None, 5, 5, 128)    0           Block17_10_Branch_0_Conv2d_1x1_Ba\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Branch_1_Conv2d_0c_7 (None, 5, 5, 128)    0           Block17_10_Branch_1_Conv2d_0c_7x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Concatenate (Concate (None, 5, 5, 256)    0           Block17_10_Branch_0_Conv2d_1x1_Ac\n",
      "                                                                 Block17_10_Branch_1_Conv2d_0c_7x1\n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Conv2d_1x1 (Conv2D)  (None, 5, 5, 896)    230272      Block17_10_Concatenate[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 5, 5, 896)    0           Block17_10_Conv2d_1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 5, 5, 896)    0           Block17_9_Activation[0][0]       \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block17_10_Activation (Activati (None, 5, 5, 896)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1 (None, 5, 5, 256)    229376      Block17_10_Activation[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1 (None, 5, 5, 256)    768         Mixed_7a_Branch_2_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1 (None, 5, 5, 256)    0           Mixed_7a_Branch_2_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1 (None, 5, 5, 256)    229376      Block17_10_Activation[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1 (None, 5, 5, 256)    229376      Block17_10_Activation[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3 (None, 5, 5, 256)    589824      Mixed_7a_Branch_2_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1 (None, 5, 5, 256)    768         Mixed_7a_Branch_0_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1 (None, 5, 5, 256)    768         Mixed_7a_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3 (None, 5, 5, 256)    768         Mixed_7a_Branch_2_Conv2d_0b_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1 (None, 5, 5, 256)    0           Mixed_7a_Branch_0_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1 (None, 5, 5, 256)    0           Mixed_7a_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3 (None, 5, 5, 256)    0           Mixed_7a_Branch_2_Conv2d_0b_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3 (None, 2, 2, 384)    884736      Mixed_7a_Branch_0_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3 (None, 2, 2, 256)    589824      Mixed_7a_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3 (None, 2, 2, 256)    589824      Mixed_7a_Branch_2_Conv2d_0b_3x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3 (None, 2, 2, 384)    1152        Mixed_7a_Branch_0_Conv2d_1a_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3 (None, 2, 2, 256)    768         Mixed_7a_Branch_1_Conv2d_1a_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3 (None, 2, 2, 256)    768         Mixed_7a_Branch_2_Conv2d_1a_3x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3 (None, 2, 2, 384)    0           Mixed_7a_Branch_0_Conv2d_1a_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3 (None, 2, 2, 256)    0           Mixed_7a_Branch_1_Conv2d_1a_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3 (None, 2, 2, 256)    0           Mixed_7a_Branch_2_Conv2d_1a_3x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a_Branch_3_MaxPool_1a_3x (None, 2, 2, 896)    0           Block17_10_Activation[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Mixed_7a (Concatenate)          (None, 2, 2, 1792)   0           Mixed_7a_Branch_0_Conv2d_1a_3x3_A\n",
      "                                                                 Mixed_7a_Branch_1_Conv2d_1a_3x3_A\n",
      "                                                                 Mixed_7a_Branch_2_Conv2d_1a_3x3_A\n",
      "                                                                 Mixed_7a_Branch_3_MaxPool_1a_3x3[\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_1_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_1_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_1_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_1_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_1_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_1_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_1_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_1_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_1_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_1_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_1_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_1_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_1_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 2, 2, 1792)   0           Block8_1_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 2, 2, 1792)   0           Mixed_7a[0][0]                   \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block8_1_Activation (Activation (None, 2, 2, 1792)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Block8_1_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_2_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_2_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_2_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_2_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_2_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Block8_1_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_2_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_2_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_2_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_2_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_2_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_2_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_2_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_2_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 2, 2, 1792)   0           Block8_2_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 2, 2, 1792)   0           Block8_1_Activation[0][0]        \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block8_2_Activation (Activation (None, 2, 2, 1792)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Block8_2_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_3_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_3_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_3_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_3_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_3_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Block8_2_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_3_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_3_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_3_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_3_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_3_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_3_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_3_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_3_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 2, 2, 1792)   0           Block8_3_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 2, 2, 1792)   0           Block8_2_Activation[0][0]        \n",
      "                                                                 lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block8_3_Activation (Activation (None, 2, 2, 1792)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Block8_3_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_4_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_4_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_4_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_4_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_4_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Block8_3_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_4_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_4_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_4_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_4_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_4_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_4_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_4_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_4_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 2, 2, 1792)   0           Block8_4_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 2, 2, 1792)   0           Block8_3_Activation[0][0]        \n",
      "                                                                 lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block8_4_Activation (Activation (None, 2, 2, 1792)   0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Block8_4_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_5_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_5_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_5_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_5_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_5_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Block8_4_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_5_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_5_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_5_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_5_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_5_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_5_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_5_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_5_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 2, 2, 1792)   0           Block8_5_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 2, 2, 1792)   0           Block8_4_Activation[0][0]        \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block8_5_Activation (Activation (None, 2, 2, 1792)   0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    344064      Block8_5_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    576         Block8_6_Branch_1_Conv2d_0a_1x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1 (None, 2, 2, 192)    0           Block8_6_Branch_1_Conv2d_0a_1x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    110592      Block8_6_Branch_1_Conv2d_0a_1x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    576         Block8_6_Branch_1_Conv2d_0b_1x3[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3 (None, 2, 2, 192)    0           Block8_6_Branch_1_Conv2d_0b_1x3_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_0_Conv2d_1x1 (C (None, 2, 2, 192)    344064      Block8_5_Activation[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    110592      Block8_6_Branch_1_Conv2d_0b_1x3_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_0_Conv2d_1x1_Ba (None, 2, 2, 192)    576         Block8_6_Branch_0_Conv2d_1x1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    576         Block8_6_Branch_1_Conv2d_0c_3x1[0\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_0_Conv2d_1x1_Ac (None, 2, 2, 192)    0           Block8_6_Branch_0_Conv2d_1x1_Batc\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1 (None, 2, 2, 192)    0           Block8_6_Branch_1_Conv2d_0c_3x1_B\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Concatenate (Concatena (None, 2, 2, 384)    0           Block8_6_Branch_0_Conv2d_1x1_Acti\n",
      "                                                                 Block8_6_Branch_1_Conv2d_0c_3x1_A\n",
      "__________________________________________________________________________________________________\n",
      "Block8_6_Conv2d_1x1 (Conv2D)    (None, 2, 2, 1792)   689920      Block8_6_Concatenate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 2, 2, 1792)   0           Block8_6_Conv2d_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 2, 2, 1792)   0           Block8_5_Activation[0][0]        \n",
      "                                                                 lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (GlobalAveragePooling2D (None, 1792)         0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Dropout (Dropout)               (None, 1792)         0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Bottleneck (Dense)              (None, 128)          229376      Dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Bottleneck_BatchNorm (BatchNorm (None, 128)          384         Bottleneck[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "normalize (Lambda)              (None, 128)          0           Bottleneck_BatchNorm[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 22,808,144\n",
      "Trainable params: 22,779,312\n",
      "Non-trainable params: 28,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inception_resnet_v1 (Model)     (None, 128)          22808144    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 128)          0           inception_resnet_v1[1][0]        \n",
      "                                                                 inception_resnet_v1[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 128)          0           inception_resnet_v1[1][0]        \n",
      "                                                                 inception_resnet_v1[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1)            0           subtract_1[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           lambda_22[0][0]                  \n",
      "                                                                 lambda_22[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 22,808,144\n",
      "Trainable params: 22,779,312\n",
      "Non-trainable params: 28,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (120,120,3)\n",
    "\n",
    "feature_model = InceptionResNetV1(input_shape=input_shape)\n",
    "feature_model.summary()\n",
    "\n",
    "x1 = Input(input_shape)\n",
    "x2 = Input(input_shape)\n",
    "x3 = Input(input_shape)\n",
    "\n",
    "v1 = feature_model(x1)\n",
    "v2 = feature_model(x2)\n",
    "v3 = feature_model(x3)\n",
    "\n",
    "p = Subtract()([v1, v2])\n",
    "n = Subtract()([v1, v3])\n",
    "\n",
    "euclidean_distance = Lambda(lambda x: K.sqrt(K.sum(K.square(x), axis=-1, keepdims=True)))\n",
    "d_p = euclidean_distance(p)\n",
    "d_n = euclidean_distance(n)\n",
    "d = Concatenate(axis=-1)([d_p, d_n])\n",
    "\n",
    "model = Model([x1, x2, x3], d)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 120, 120, 3)\n",
      "(50,)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "data_path = '../data/mask/face dataset/*'\n",
    "data_dir_paths = glob.glob(data_path)\n",
    "\n",
    "index = [0]\n",
    "count = []\n",
    "images = []\n",
    "for dir_path in data_dir_paths:\n",
    "    image_paths = glob.glob(os.path.join(dir_path,'*'))\n",
    "    index.append(len(image_paths) + index[-1])\n",
    "    count.append(len(image_paths))\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        images.append(image)\n",
    "del index[-1]\n",
    "images = np.array(images)\n",
    "index = np.array(index)\n",
    "count = np.array(count)\n",
    "\n",
    "print(images.shape)\n",
    "print(index.shape)\n",
    "print(count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP_2\\anaconda3\\envs\\hhs\\lib\\site-packages\\keras\\utils\\io_utils.py:60: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(datapath)\n"
     ]
    }
   ],
   "source": [
    "gen_train = DataGenerator(data_path='../data/db_cropped_face/data.h5', data_type='train', batch_size=100)\n",
    "gen_val = DataGenerator(data_path='../data/db_cropped_face/data.h5', data_type='val', batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1914/1914 [==============================] - 1187s 620ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02068, saving model to check_point(best).h5\n",
      "Epoch 2/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 0.0092 - val_loss: 0.0248\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.02068\n",
      "Epoch 3/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 0.0062 - val_loss: 0.0268\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02068\n",
      "Epoch 4/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 0.0048 - val_loss: 0.0330\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02068\n",
      "Epoch 5/120\n",
      "1914/1914 [==============================] - 1140s 595ms/step - loss: 0.0034 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02068\n",
      "Epoch 6/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 0.0026 - val_loss: 0.0348\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02068\n",
      "Epoch 7/120\n",
      "1914/1914 [==============================] - 1140s 595ms/step - loss: 0.0022 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02068\n",
      "Epoch 8/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 0.0019 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02068 to 0.01229, saving model to check_point(best).h5\n",
      "Epoch 9/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 0.0015 - val_loss: 0.0117\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01229 to 0.01167, saving model to check_point(best).h5\n",
      "Epoch 10/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 0.0013 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01167\n",
      "Epoch 11/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 0.0012 - val_loss: 0.0231\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01167\n",
      "Epoch 12/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 0.0011 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01167\n",
      "Epoch 13/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 9.2607e-04 - val_loss: 0.0308\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01167\n",
      "Epoch 14/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 8.2399e-04 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01167 to 0.01074, saving model to check_point(best).h5\n",
      "Epoch 15/120\n",
      "1914/1914 [==============================] - 1137s 594ms/step - loss: 8.5492e-04 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01074\n",
      "Epoch 16/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 8.4510e-04 - val_loss: 0.0332\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01074\n",
      "Epoch 17/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 7.9113e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01074\n",
      "Epoch 18/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 7.3738e-04 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01074 to 0.00655, saving model to check_point(best).h5\n",
      "Epoch 19/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 7.3150e-04 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00655\n",
      "Epoch 20/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 6.8908e-04 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00655\n",
      "Epoch 21/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 5.8370e-04 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00655\n",
      "Epoch 22/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 3.4861e-04 - val_loss: 0.0312\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00655\n",
      "Epoch 23/120\n",
      "1914/1914 [==============================] - 1142s 596ms/step - loss: 2.6033e-04 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00655\n",
      "Epoch 24/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 2.3176e-04 - val_loss: 0.0075\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00655\n",
      "Epoch 25/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 2.2397e-04 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00655 to 0.00384, saving model to check_point(best).h5\n",
      "Epoch 26/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 2.3095e-04 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00384\n",
      "Epoch 27/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.9947e-04 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00384\n",
      "Epoch 28/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 2.0604e-04 - val_loss: 0.0339\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00384\n",
      "Epoch 29/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 2.0206e-04 - val_loss: 0.0421\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00384\n",
      "Epoch 30/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 2.1476e-04 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00384 to 0.00218, saving model to check_point(best).h5\n",
      "Epoch 31/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.7145e-04 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00218\n",
      "Epoch 32/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.9486e-04 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00218\n",
      "Epoch 33/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.7687e-04 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00218\n",
      "Epoch 34/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.5431e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00218\n",
      "Epoch 35/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.6696e-04 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00218\n",
      "Epoch 36/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.9119e-04 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00218\n",
      "Epoch 37/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.3018e-04 - val_loss: 0.0141\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00218\n",
      "Epoch 38/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.3128e-04 - val_loss: 0.0128\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00218\n",
      "Epoch 39/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.5042e-04 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00218\n",
      "Epoch 40/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.5488e-04 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00218\n",
      "Epoch 41/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.5270e-04 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00218\n",
      "Epoch 42/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.6291e-04 - val_loss: 0.0133\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00218\n",
      "Epoch 43/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.4780e-04 - val_loss: 0.0307\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00218\n",
      "Epoch 44/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.6532e-04 - val_loss: 0.0137\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00218\n",
      "Epoch 45/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 1.4702e-04 - val_loss: 0.0287\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00218\n",
      "Epoch 46/120\n",
      "1914/1914 [==============================] - 1142s 596ms/step - loss: 1.8266e-04 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00218\n",
      "Epoch 47/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.3670e-04 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00218\n",
      "Epoch 48/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.5415e-04 - val_loss: 0.0128\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00218\n",
      "Epoch 49/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.4457e-04 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00218\n",
      "Epoch 50/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.6699e-04 - val_loss: 0.0146\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00218\n",
      "Epoch 51/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.5207e-04 - val_loss: 0.0418\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00218\n",
      "Epoch 52/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.3063e-04 - val_loss: 0.0209\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00218\n",
      "Epoch 53/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.3774e-04 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00218\n",
      "Epoch 54/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.6827e-04 - val_loss: 0.0308\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00218\n",
      "Epoch 55/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.3385e-04 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00218\n",
      "Epoch 56/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 1.4828e-04 - val_loss: 0.0227\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00218\n",
      "Epoch 57/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.4503e-04 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00218\n",
      "Epoch 58/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.4576e-04 - val_loss: 0.0110\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00218\n",
      "Epoch 59/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 1.4205e-04 - val_loss: 0.0141\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00218\n",
      "Epoch 60/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.1006e-04 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00218\n",
      "Epoch 61/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.4260e-04 - val_loss: 0.0114\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00218\n",
      "Epoch 62/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.4341e-04 - val_loss: 0.0146\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00218\n",
      "Epoch 63/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.3696e-04 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00218\n",
      "Epoch 64/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.2580e-04 - val_loss: 0.0143\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00218\n",
      "Epoch 65/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.2621e-04 - val_loss: 0.0077\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00218\n",
      "Epoch 66/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.5821e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00218\n",
      "Epoch 67/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.5558e-04 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00218\n",
      "Epoch 68/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.5015e-04 - val_loss: 0.0284\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00218\n",
      "Epoch 69/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3027e-04 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00218\n",
      "Epoch 70/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 1.5068e-04 - val_loss: 0.0280\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00218\n",
      "Epoch 71/120\n",
      "1914/1914 [==============================] - 1144s 597ms/step - loss: 1.6187e-04 - val_loss: 0.0151\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00218\n",
      "Epoch 72/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3404e-04 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00218\n",
      "Epoch 73/120\n",
      "1914/1914 [==============================] - 1142s 596ms/step - loss: 1.3297e-04 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00218\n",
      "Epoch 74/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3219e-04 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00218\n",
      "Epoch 75/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.2298e-04 - val_loss: 0.0122\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00218\n",
      "Epoch 76/120\n",
      "1914/1914 [==============================] - 1141s 596ms/step - loss: 1.3310e-04 - val_loss: 0.0258\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00218\n",
      "Epoch 77/120\n",
      "1914/1914 [==============================] - 1139s 595ms/step - loss: 1.3983e-04 - val_loss: 0.0129\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00218\n",
      "Epoch 78/120\n",
      "1914/1914 [==============================] - 1140s 596ms/step - loss: 1.6033e-04 - val_loss: 0.0131\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00218\n",
      "Epoch 79/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.4586e-04 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00218\n",
      "Epoch 80/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.4049e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00218\n",
      "Epoch 81/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3468e-04 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00218\n",
      "Epoch 82/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3886e-04 - val_loss: 0.0109\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00218\n",
      "Epoch 83/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.3234e-04 - val_loss: 0.0119\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00218\n",
      "Epoch 84/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3916e-04 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00218\n",
      "Epoch 85/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.1437e-04 - val_loss: 0.0289\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00218\n",
      "Epoch 86/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.6090e-04 - val_loss: 0.0092\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00218\n",
      "Epoch 87/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3264e-04 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00218\n",
      "Epoch 88/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.5958e-04 - val_loss: 0.0237\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00218\n",
      "Epoch 89/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.5306e-04 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00218\n",
      "Epoch 90/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.3819e-04 - val_loss: 0.0223\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00218\n",
      "Epoch 91/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.2753e-04 - val_loss: 0.0219\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00218\n",
      "Epoch 92/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.3749e-04 - val_loss: 0.0214\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00218\n",
      "Epoch 93/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.3935e-04 - val_loss: 0.0151\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00218\n",
      "Epoch 94/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.4263e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00218\n",
      "Epoch 95/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.3507e-04 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00218\n",
      "Epoch 96/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.5043e-04 - val_loss: 0.0117\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00218\n",
      "Epoch 97/120\n",
      "1914/1914 [==============================] - 1147s 599ms/step - loss: 1.3369e-04 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00218\n",
      "Epoch 98/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.2431e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00218\n",
      "Epoch 99/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.6217e-04 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00218\n",
      "Epoch 100/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.3726e-04 - val_loss: 0.0082\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00218\n",
      "Epoch 101/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.4127e-04 - val_loss: 0.0117\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00218\n",
      "Epoch 102/120\n",
      "1914/1914 [==============================] - 1147s 599ms/step - loss: 1.4440e-04 - val_loss: 0.0150\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00218\n",
      "Epoch 103/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.3106e-04 - val_loss: 0.0248\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00218\n",
      "Epoch 104/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.4718e-04 - val_loss: 0.0083\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00218\n",
      "Epoch 105/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.3877e-04 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00218\n",
      "Epoch 106/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.3028e-04 - val_loss: 0.0121\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00218\n",
      "Epoch 107/120\n",
      "1914/1914 [==============================] - 1147s 599ms/step - loss: 1.5596e-04 - val_loss: 0.0082\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00218\n",
      "Epoch 108/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.4093e-04 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00218\n",
      "Epoch 109/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.2323e-04 - val_loss: 0.0246\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00218\n",
      "Epoch 110/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.2967e-04 - val_loss: 0.0307\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00218\n",
      "Epoch 111/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.3484e-04 - val_loss: 0.0299\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00218\n",
      "Epoch 112/120\n",
      "1914/1914 [==============================] - 1145s 598ms/step - loss: 1.1536e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00218\n",
      "Epoch 113/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.4111e-04 - val_loss: 0.0119\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00218\n",
      "Epoch 114/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.5774e-04 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00218\n",
      "Epoch 115/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.4435e-04 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00218\n",
      "Epoch 116/120\n",
      "1914/1914 [==============================] - 1143s 597ms/step - loss: 1.1734e-04 - val_loss: 0.0109\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00218\n",
      "Epoch 117/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.4457e-04 - val_loss: 0.0258\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00218\n",
      "Epoch 118/120\n",
      "1914/1914 [==============================] - 1144s 598ms/step - loss: 1.4176e-04 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00218\n",
      "Epoch 119/120\n",
      "1914/1914 [==============================] - 1142s 597ms/step - loss: 1.4331e-04 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00218\n",
      "Epoch 120/120\n",
      "1914/1914 [==============================] - 1146s 599ms/step - loss: 1.4018e-04 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00218\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hist_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-42ad134a60e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_path' is not defined"
     ]
    }
   ],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = 0.2\n",
    "    loss = K.mean(K.maximum(0.0, K.square(y_pred[:, 0]) - K.square(y_pred[:, 1]) + margin))\n",
    "    return loss\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 0.001\n",
    "    if epoch > 20: lr /= 10\n",
    "    if epoch > 40: lr /= 10\n",
    "    if epoch > 60: lr /= 10\n",
    "    if epoch > 80: lr /= 10\n",
    "    if epoch > 100: lr /= 10\n",
    "    return lr\n",
    "\n",
    "scheduler = LearningRateScheduler(lr_schedule)\n",
    "check_point = ModelCheckpoint(filepath='check_point(best).h5', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [scheduler, check_point]\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss=triplet_loss)\n",
    "history = model.fit_generator(generator=gen_train, epochs=120, validation_data=gen_val, callbacks=callbacks)\n",
    "\n",
    "model.save('model.h5', include_optimizer=False)\n",
    "with open(hist_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show and save history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('hist', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 120, 120, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inception_resnet_v1 (Model)     (None, 128)          22808144    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 128)          0           inception_resnet_v1[1][0]        \n",
      "                                                                 inception_resnet_v1[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 128)          0           inception_resnet_v1[1][0]        \n",
      "                                                                 inception_resnet_v1[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1)            0           subtract_1[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           lambda_22[0][0]                  \n",
      "                                                                 lambda_22[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 22,808,144\n",
      "Trainable params: 22,779,312\n",
      "Non-trainable params: 28,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../result/model/model.h5', compile=False, custom_objects={'tf': tf})\n",
    "model.summary()\n",
    "\n",
    "# gen_test = DataGenerator(data_path='../data/db_cropped_face/data.h5', data_type='test', batch_size=2663)\n",
    "gen_test = DataGenerator(images, index, count, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th / 15\n",
      "1th / 15\n",
      "2th / 15\n",
      "3th / 15\n",
      "4th / 15\n",
      "5th / 15\n",
      "6th / 15\n",
      "7th / 15\n",
      "8th / 15\n",
      "9th / 15\n",
      "10th / 15\n",
      "11th / 15\n",
      "12th / 15\n",
      "13th / 15\n",
      "14th / 15\n",
      "(750,)\n",
      "(750,)\n",
      "0.008207713 1.614831\n",
      "0.030854976 1.7296044\n"
     ]
    }
   ],
   "source": [
    "genuine_pair = []\n",
    "imposter_pair = []\n",
    "for i in range(gen_test.__len__()):\n",
    "    print('%dth / %d'%(i, gen_test.__len__()))\n",
    "    xs, _ = gen_test.__getitem__(i)\n",
    "    pred = model.predict(xs)\n",
    "    genuine_pair.append(pred[:, 0])\n",
    "    imposter_pair.append(pred[:, 1])\n",
    "genuine_pair = np.array(genuine_pair).flatten()\n",
    "imposter_pair = np.array(imposter_pair).flatten()\n",
    "np.save('mask_genuine', genuine_pair)\n",
    "np.save('mask_imposter', imposter_pair)\n",
    "\n",
    "print(genuine_pair.shape)\n",
    "print(imposter_pair.shape)\n",
    "print(genuine_pair.min(), genuine_pair.max())\n",
    "print(imposter_pair.min(), imposter_pair.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.7023230686115612\n",
      "Accuracy :  0.6546666666666666\n"
     ]
    }
   ],
   "source": [
    "max_f1_score = 0\n",
    "max_accuracy = 0\n",
    "for threshold in np.arange(0, 2, 0.001):\n",
    "    tp = np.sum(genuine_pair < threshold)\n",
    "    fp = np.sum(imposter_pair < threshold)\n",
    "    fn = np.sum(genuine_pair >= threshold)\n",
    "    tn = np.sum(imposter_pair >= threshold)\n",
    "    \n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        precision = tp / (tp+fp)\n",
    "        recall = tp / (tp+fn)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + fn + fp + tn)\n",
    "    \n",
    "    if f1_score > max_f1_score:\n",
    "        max_f1_score = f1_score\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        \n",
    "print('F1 score : ', max_f1_score)\n",
    "print('Accuracy : ', max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genuine imposter histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(genuine_pair, bins=1000)\n",
    "plt.hist(imposter_pair, bins=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
